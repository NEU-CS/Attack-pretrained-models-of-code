{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset,load_dataset\n",
    "import json\n",
    "path = os.path.join(\"..\",\"..\",\"..\",\"data\")\n",
    "data = load_dataset(\"json\",data_files={\"train\":os.path.join(path,\"train_java2cs.jsonl\"),\"test\":os.path.join(path,\"valid_java2cs.jsonl\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "basemodel = \"Salesforce/codet5-base\"\n",
    "tokenzier = AutoTokenizer.from_pretrained(basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcelg = \"java\"\n",
    "tgtlg = \"cs\"\n",
    "prefix = \"\"\"#translate this java code to c-sharp code:\n",
    "java:\"\"\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + exmaple[sourcelg] for exmaple in examples['translation']]\n",
    "    tgts = [example[tgtlg] for example in examples['translation']]\n",
    "    model_inputs = tokenzier(inputs,text_target=tgts,max_length = 256,truncation= True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930f7af2b5dd46a0ab4375c470165939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenzied_data = data.map(preprocess_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenzier,model = basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bleu\n",
    "import dataflow_match\n",
    "import syntax_match\n",
    "import weighted_ngram_match\n",
    "import numpy as np\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()for label in labels]]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    decoded_preds = tokenzier.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenzier.pad_token_id)\n",
    "    decoded_labels = tokenzier.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    hypothesis, pre_references = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    for i in range(len(pre_references)):\n",
    "        assert len(hypothesis) == len(pre_references[i])\n",
    "\n",
    "    references = []\n",
    "    for i in range(len(hypothesis)):\n",
    "        ref_for_instance = []\n",
    "        for j in range(len(pre_references)):\n",
    "            ref_for_instance.append(pre_references[j][i])\n",
    "        references.append(ref_for_instance)\n",
    "    assert len(references) == len(pre_references)*len(hypothesis)\n",
    "\n",
    "\n",
    "    tokenized_hyps = [x.split() for x in hypothesis]\n",
    "    tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "\n",
    "    # calculate weighted ngram match\n",
    "    keywords = [x.strip() for x in open('keywords/'+\"c_sharp\"+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "    def make_weights(reference_tokens, key_word_list):\n",
    "        return {token:1 if token in key_word_list else 0.2 \\\n",
    "                for token in reference_tokens}\n",
    "    tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "                for reference_tokens in reference] for reference in tokenized_refs]\n",
    "    \n",
    "    result = {}\n",
    "    result['BLEU'] = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "    result['Weighted_BLEU'] = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "    result['Syntax Match accuracy'] = syntax_match.corpus_syntax_match(references,hypothesis,\"c_sharp\")\n",
    "    result['Dataflow_match accuracy'] = dataflow_match.corpus_dataflow_match(references,hypothesis,\"c_sharp\")\n",
    "    result['CodeBLEU'] = 0.25*result['BLEU'] + 0.25 * result['Weighted_BLEU'] + 0.25 * result['Syntax Match accuracy'] + 0.25 * result['Dataflow_match accuracy']\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM,Seq2SeqTrainingArguments,Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_nums = 20\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"CodeT5ForCodeTrans\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=epoch_nums,\n",
    "    predict_with_generate=True,\n",
    "    warmup_steps=0.06 * (len(tokenzied_data['train']) * epoch_nums),\n",
    "    fp16=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m1364729682\u001b[0m (\u001b[33mljcnju\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9abe43109d491bbc59b389ca958d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113292268580861, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ljc/desktop/blackbox-attack-for-code-pretrained-models/CodeT5/CodeTrans/code/finetune/CodeBLEU/wandb/run-20240130_190651-y1t5z5py</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ljcnju/huggingface/runs/y1t5z5py' target=\"_blank\">atomic-violet-156</a></strong> to <a href='https://wandb.ai/ljcnju/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ljcnju/huggingface' target=\"_blank\">https://wandb.ai/ljcnju/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ljcnju/huggingface/runs/y1t5z5py' target=\"_blank\">https://wandb.ai/ljcnju/huggingface/runs/y1t5z5py</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6420' max='6420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6420/6420 45:05, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Weighted Bleu</th>\n",
       "      <th>Syntax match accuracy</th>\n",
       "      <th>Dataflow Match accuracy</th>\n",
       "      <th>Codebleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.382000</td>\n",
       "      <td>2.857626</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.004617</td>\n",
       "      <td>0.106286</td>\n",
       "      <td>0.040971</td>\n",
       "      <td>0.038165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.788400</td>\n",
       "      <td>0.973179</td>\n",
       "      <td>0.048843</td>\n",
       "      <td>0.085919</td>\n",
       "      <td>0.177985</td>\n",
       "      <td>0.085498</td>\n",
       "      <td>0.099561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>0.540023</td>\n",
       "      <td>0.129351</td>\n",
       "      <td>0.203648</td>\n",
       "      <td>0.199940</td>\n",
       "      <td>0.072047</td>\n",
       "      <td>0.151246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>0.381767</td>\n",
       "      <td>0.150047</td>\n",
       "      <td>0.234439</td>\n",
       "      <td>0.213774</td>\n",
       "      <td>0.069728</td>\n",
       "      <td>0.166997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.411100</td>\n",
       "      <td>0.342289</td>\n",
       "      <td>0.156073</td>\n",
       "      <td>0.243483</td>\n",
       "      <td>0.210947</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.169787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>0.314565</td>\n",
       "      <td>0.161879</td>\n",
       "      <td>0.251916</td>\n",
       "      <td>0.212752</td>\n",
       "      <td>0.066790</td>\n",
       "      <td>0.173334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.332800</td>\n",
       "      <td>0.275700</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>0.259106</td>\n",
       "      <td>0.214195</td>\n",
       "      <td>0.066481</td>\n",
       "      <td>0.176385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.259337</td>\n",
       "      <td>0.166210</td>\n",
       "      <td>0.260426</td>\n",
       "      <td>0.214075</td>\n",
       "      <td>0.066790</td>\n",
       "      <td>0.176875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.286200</td>\n",
       "      <td>0.246577</td>\n",
       "      <td>0.169831</td>\n",
       "      <td>0.265374</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.066636</td>\n",
       "      <td>0.179415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.253200</td>\n",
       "      <td>0.226237</td>\n",
       "      <td>0.169315</td>\n",
       "      <td>0.265898</td>\n",
       "      <td>0.216241</td>\n",
       "      <td>0.067254</td>\n",
       "      <td>0.179677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.215035</td>\n",
       "      <td>0.170664</td>\n",
       "      <td>0.267884</td>\n",
       "      <td>0.217263</td>\n",
       "      <td>0.066945</td>\n",
       "      <td>0.180689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.207426</td>\n",
       "      <td>0.170406</td>\n",
       "      <td>0.268474</td>\n",
       "      <td>0.217684</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.180916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.193839</td>\n",
       "      <td>0.174429</td>\n",
       "      <td>0.274393</td>\n",
       "      <td>0.220030</td>\n",
       "      <td>0.067718</td>\n",
       "      <td>0.184143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.205900</td>\n",
       "      <td>0.185733</td>\n",
       "      <td>0.173291</td>\n",
       "      <td>0.273327</td>\n",
       "      <td>0.220752</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.183888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.181589</td>\n",
       "      <td>0.175294</td>\n",
       "      <td>0.276223</td>\n",
       "      <td>0.220932</td>\n",
       "      <td>0.067563</td>\n",
       "      <td>0.185003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.182600</td>\n",
       "      <td>0.175912</td>\n",
       "      <td>0.175194</td>\n",
       "      <td>0.275943</td>\n",
       "      <td>0.220391</td>\n",
       "      <td>0.069109</td>\n",
       "      <td>0.185159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ljc/miniconda3/envs/adv/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6420, training_loss=0.5954580588504161, metrics={'train_runtime': 2713.5579, 'train_samples_per_second': 75.878, 'train_steps_per_second': 2.366, 'total_flos': 3.330567206111232e+16, 'train_loss': 0.5954580588504161, 'epoch': 19.95})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenzied_data[\"train\"],\n",
    "    eval_dataset=tokenzied_data[\"test\"],\n",
    "    tokenizer=tokenzier,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"CodeT5ForCodeTrans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CodeT5ForCodeTrans/tokenizer_config.json',\n",
       " 'CodeT5ForCodeTrans/special_tokens_map.json',\n",
       " 'CodeT5ForCodeTrans/vocab.json',\n",
       " 'CodeT5ForCodeTrans/merges.txt',\n",
       " 'CodeT5ForCodeTrans/added_tokens.json',\n",
       " 'CodeT5ForCodeTrans/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenzier.save_pretrained(\"CodeT5ForCodeTrans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
