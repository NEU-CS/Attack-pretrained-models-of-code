{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM,pipeline,RobertaForMaskedLM,RobertaTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams,EngineArgs, LLMEngine, RequestOutput\n",
    "from vllm.lora.request import LoRARequest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 16:24:36 llm_engine.py:72] Initializing an LLM engine with config: model='deepseek-ai/deepseek-coder-6.7b-base', tokenizer='deepseek-ai/deepseek-coder-6.7b-base', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 16:24:39 weight_utils.py:164] Using model weights format ['*.bin']\n",
      "INFO 02-28 16:24:47 llm_engine.py:322] # GPU blocks: 503, # CPU blocks: 512\n",
      "INFO 02-28 16:24:48 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-28 16:24:48 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-28 16:24:53 model_runner.py:698] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "engine_args = EngineArgs(model=\"deepseek-ai/deepseek-coder-6.7b-base\",\n",
    "                             enable_lora=True,\n",
    "                             max_loras=1,\n",
    "                             max_lora_rank=8,\n",
    "                             max_cpu_loras=2,\n",
    "                             max_num_seqs=256,\n",
    "                             max_model_len= 512)\n",
    "engine = LLMEngine.from_engine_args(engine_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorarequest = LoRARequest(\"DeepSeek7bForCodeTrans\",1,\"DeepSeek7bForCodeTrans/checkpoint-6435\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.add_lora(lorarequest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_java_code|>public void serialize(LittleEndianOutput out) {out.writeShort(field_1_vcenter);}\\n<|end_of_java_code|><|begin_of_c-sharp_code|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_special_tokens = {'additional_special_tokens':['<|begin_of_java_code|>','<|end_of_java_code|>'\\\n",
    "                                                           ,'<|begin_of_c-sharp_code|>','<|end_of_c-sharp_code|>',\\\n",
    "                                                            '<|translate|>']}\n",
    "prompt = \"public void serialize(LittleEndianOutput out) {out.writeShort(field_1_vcenter);}\\n\"\n",
    "prompt = additional_special_tokens['additional_special_tokens'][0] + prompt + additional_special_tokens['additional_special_tokens'][1] + additional_special_tokens['additional_special_tokens'][2]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.1,max_tokens= 512,stop_token_ids=[32022,32014],skip_special_tokens=False)\n",
    "engine.add_request(str(3),prompt,sampling_params,lora_request=lorarequest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 16:24:57 llm_engine.py:877] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=3, prompt='<|begin_of_java_code|>public void serialize(LittleEndianOutput out) {out.writeShort(field_1_vcenter);}\\n<|end_of_java_code|><|begin_of_c-sharp_code|>', prompt_token_ids=[32013, 32022, 3566, 2494, 11071, 938, 7, 29329, 6015, 987, 8780, 631, 8, 507, 406, 13, 6449, 19721, 7, 3267, 62, 16, 62, 85, 4679, 477, 92, 185, 32023, 32024], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='public', token_ids=[3566], cumulative_logprob=0.0, logprobs=None, finish_reason=None)], finished=False, lora_request=LoRARequest(lora_name='DeepSeek7bForCodeTrans', lora_int_id=1, lora_local_path='DeepSeek7bForCodeTrans/checkpoint-6435'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 16:31:15 llm_engine.py:877] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "real_output = \"\"\n",
    "finished = False\n",
    "while engine.has_unfinished_requests():\n",
    "    request_outputs  =  engine.step()\n",
    "    for request_output in request_outputs:\n",
    "        finished = finished | request_output.finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3566,\n",
       " 6632,\n",
       " 5891,\n",
       " 14361,\n",
       " 5349,\n",
       " 50,\n",
       " 5492,\n",
       " 19467,\n",
       " 31324,\n",
       " 6522,\n",
       " 5891,\n",
       " 14361,\n",
       " 5349,\n",
       " 50,\n",
       " 5492,\n",
       " 19467,\n",
       " 31324,\n",
       " 7,\n",
       " 2245,\n",
       " 14361,\n",
       " 5349,\n",
       " 50,\n",
       " 5492,\n",
       " 19467,\n",
       " 31324,\n",
       " 4397,\n",
       " 3092,\n",
       " 4546,\n",
       " 1881,\n",
       " 3438,\n",
       " 405,\n",
       " 756,\n",
       " 21968,\n",
       " 3507,\n",
       " 7878,\n",
       " 1293,\n",
       " 6732,\n",
       " 13,\n",
       " 4397,\n",
       " 9468,\n",
       " 11862,\n",
       " 250,\n",
       " 405,\n",
       " 5891,\n",
       " 14361,\n",
       " 5349,\n",
       " 50,\n",
       " 5492,\n",
       " 19467,\n",
       " 31324,\n",
       " 4397,\n",
       " 9468,\n",
       " 11862,\n",
       " 250,\n",
       " 13,\n",
       " 7552,\n",
       " 26,\n",
       " 6732,\n",
       " 13,\n",
       " 6522,\n",
       " 2826,\n",
       " 3957,\n",
       " 11862,\n",
       " 250,\n",
       " 405,\n",
       " 5891,\n",
       " 14361,\n",
       " 5349,\n",
       " 50,\n",
       " 5492,\n",
       " 19467,\n",
       " 31324,\n",
       " 6522,\n",
       " 2826,\n",
       " 3957,\n",
       " 11862,\n",
       " 250,\n",
       " 13,\n",
       " 7552,\n",
       " 26,\n",
       " 2125,\n",
       " 21968,\n",
       " 3507,\n",
       " 27,\n",
       " 2245,\n",
       " 14361,\n",
       " 5349,\n",
       " 50,\n",
       " 5492,\n",
       " 19467,\n",
       " 31324,\n",
       " 6522,\n",
       " 29,\n",
       " 7,\n",
       " 6555,\n",
       " 11,\n",
       " 3438,\n",
       " 477,\n",
       " 92,\n",
       " 185,\n",
       " 32022]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_outputs[0].outputs[0].token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'public virtual ListSpeechSynthesisTasksResponse ListSpeechSynthesisTasks(ListSpeechSynthesisTasksRequest request){var options = new InvokeOptions();options.RequestMarshaller = ListSpeechSynthesisTasksRequestMarshaller.Instance;options.ResponseUnmarshaller = ListSpeechSynthesisTasksResponseUnmarshaller.Instance;return Invoke<ListSpeechSynthesisTasksResponse>(request, options);}\\n<|begin_of_java_code|>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|translate|>public ListSpeechSynthesisTasksResult listSpeechSynthesisTasks(ListSpeechSynthesisTasksRequest request) {request = beforeClientExecution(request);return executeListSpeechSynthesisTasks(request);}\\n<|end_of_c-sharp_code|><|begin_of_c-sharp_code|>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_special_tokens = {'additional_special_tokens':['<|begin_of_java_code|>','<|end_of_java_code|>'\\\n",
    "                                                           ,'<|begin_of_c-sharp_code|>','<|end_of_c-sharp_code|>',\\\n",
    "                                                            '<|translate|>']}\n",
    "prompt = \"public ListSpeechSynthesisTasksResult listSpeechSynthesisTasks(ListSpeechSynthesisTasksRequest request) {request = beforeClientExecution(request);return executeListSpeechSynthesisTasks(request);}\\n\"\n",
    "prompt = additional_special_tokens['additional_special_tokens'][-1] + prompt + additional_special_tokens['additional_special_tokens'][3] + additional_special_tokens['additional_special_tokens'][2]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.1,max_tokens= 512,stop_token_ids=[32025,32014],skip_special_tokens=False)\n",
    "engine.add_request(str(5),prompt,sampling_params,lora_request=lorarequest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 16:31:10 llm_engine.py:877] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=5, prompt='<|translate|>public ListSpeechSynthesisTasksResult listSpeechSynthesisTasks(ListSpeechSynthesisTasksRequest request) {request = beforeClientExecution(request);return executeListSpeechSynthesisTasks(request);}\\n<|end_of_c-sharp_code|><|begin_of_c-sharp_code|>', prompt_token_ids=[32013, 32026, 3566, 5891, 14361, 5349, 50, 5492, 19467, 31324, 4835, 1517, 14361, 5349, 50, 5492, 19467, 31324, 7, 2245, 14361, 5349, 50, 5492, 19467, 31324, 4397, 3092, 8, 507, 6555, 405, 1321, 5960, 25793, 7, 6555, 477, 2125, 11682, 2245, 14361, 5349, 50, 5492, 19467, 31324, 7, 6555, 477, 92, 185, 32025, 32024], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='public', token_ids=[3566], cumulative_logprob=0.0, logprobs=None, finish_reason=None)], finished=False, lora_request=LoRARequest(lora_name='DeepSeek7bForCodeTrans', lora_int_id=1, lora_local_path='DeepSeek7bForCodeTrans/checkpoint-6435'))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_c_sharp_code(code:str):\n",
    "    codelst = code.split('<|begin_of_java_code|>')\n",
    "    return codelst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"public override void Serialize(ILittleEndianOutput out1){out1.WriteShort(field_1_vcenter);}<|begin_of_java_code|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'public override void Serialize(ILittleEndianOutput out1){out1.WriteShort(field_1_vcenter);}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_c_sharp_code(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
